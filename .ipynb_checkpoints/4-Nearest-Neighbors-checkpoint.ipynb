{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defense Against Adversarial Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import urllib\n",
    "import time\n",
    "import pickle\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WAF(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        good_query_list = self.get_query_list('URL/goodqueries.txt')\n",
    "        bad_query_list = self.get_query_list('URL/badqueries.txt')\n",
    "        \n",
    "        good_y = [0 for i in range(0,len(good_query_list))]\n",
    "        bad_y = [1 for i in range(0,len(bad_query_list))]\n",
    "\n",
    "        queries = bad_query_list+good_query_list\n",
    "        y = bad_y + good_y\n",
    "\n",
    "        #converting data to vectors  定义矢量化实例\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self.get_ngrams)\n",
    "\n",
    "        # 把不规律的文本字符串列表转换成规律的 ( [i,j],tdidf值) 的矩阵X\n",
    "        # 用于下一步训练分类器 lgs\n",
    "        X = self.vectorizer.fit_transform(queries)\n",
    "\n",
    "        # 使用 train_test_split 分割 X y 列表\n",
    "        # X_train矩阵的数目对应 y_train列表的数目(一一对应)  -->> 用来训练模型\n",
    "        # X_test矩阵的数目对应 \t (一一对应) -->> 用来测试模型的准确性\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=20, random_state=42)\n",
    "\n",
    "        # 定理逻辑回归方法模型\n",
    "        self.lgs = LogisticRegression()\n",
    "\n",
    "        # 使用逻辑回归方法训练模型实例 lgs\n",
    "        self.lgs.fit(X_train, y_train)\n",
    "\n",
    "        # 使用测试值 对 模型的准确度进行计算\n",
    "        print('模型的准确度:{}'.format(self.lgs.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-f3653f452be6>, line 44)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-f3653f452be6>\"\u001b[1;36m, line \u001b[1;32m44\u001b[0m\n\u001b[1;33m    with open('lgs.pickle','rb') as input:\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "    # 对 新的请求列表进行预测\n",
    "    def predict(self,new_queries):\n",
    "        new_queries = [urllib.parse.unquote(url) for url in new_queries]\n",
    "        X_predict = self.vectorizer.transform(new_queries)\n",
    "        res = self.lgs.predict(X_predict)\n",
    "        res_list = []\n",
    "        for q,r in zip(new_queries,res):\n",
    "            tmp = '正常请求'if r == 0 else '恶意请求'\n",
    "            # print('{}  {}'.format(q,tmp))\n",
    "            q_entity = html.escape(q)\n",
    "            res_list.append({'url':q_entity,'res':tmp})\n",
    "        print(\"预测的结果列表:{}\".format(str(res_list)))\n",
    "        return res_list\n",
    "        \n",
    "\n",
    "    # 得到文本中的请求列表\n",
    "    def get_query_list(self,filename):\n",
    "        directory = str(os.getcwd())\n",
    "        # directory = str(os.getcwd())+'/module/waf'\n",
    "        filepath = directory + \"/\" + filename\n",
    "        data = open(filepath,'r').readlines()\n",
    "        query_list = []\n",
    "        for d in data:\n",
    "            d = str(urllib.parse.unquote(d))   #converting url encoded data to simple string\n",
    "            # print(d)\n",
    "            query_list.append(d)\n",
    "        return list(set(query_list))\n",
    "\n",
    "\n",
    "    #tokenizer function, this will make 3 grams of each query\n",
    "    def get_ngrams(self,query):\n",
    "        tempQuery = str(query)\n",
    "        ngrams = []\n",
    "        for i in range(0,len(tempQuery)-3):\n",
    "            ngrams.append(tempQuery[i:i+3])\n",
    "        return ngrams\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 若 检测模型文件lgs.pickle 不存在,需要先训练出模型\n",
    "    # w = WAF()\n",
    "    # with open('lgs.pickle','wb') as output:\n",
    "    #     pickle.dump(w,output)\n",
    "\n",
    "    with open('lgs.pickle','rb') as input:\n",
    "        w = pickle.load(input)\n",
    "\n",
    "    # X has 46 features per sample; expecting 7  youqude  cuowu  \n",
    "    w.predict(['www.foo.com/id=1<script>alert(1)</script>','www.foo.com/name=admin\\' or 1=1','abc.com/admin.php',\n",
    "    '\"><svg onload=confirm(1)>','test/q=<a href=\"javascript:confirm(1)>','q=../etc/passwd'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
